services:
  ollama:
    container_name: ollama
    build:
      context: .
      dockerfile: ./Dockerfile
    ports:
      - 11434:11434
    volumes:
      - chatbot-vol:/ollama
    networks:
      - NPC
    entrypoint: [ "/usr/bin/bash", "pull_models.sh" ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # for non-swarm mode, add this instead of deploy:
    runtime: nvidia
    
networks:
  NPC:
    driver: bridge

volumes:
  chatbot-vol:
    driver: local
